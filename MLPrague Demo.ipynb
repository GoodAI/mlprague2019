{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you are on the correct conda environment (sound)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!conda info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some basic imports\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "# Change this to 'cpu' if your machine doesn't have cuda capability.\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load an audio file\n",
    "Import code for loading wav files from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from code.lib.mel_features import read_wav\n",
    "# the code below allows us to convert the wav's sample rate to a desired sample rate; 8kHz frequency maximum is OK for us\n",
    "wav_sample_rate=16000\n",
    "wav1 = read_wav(\"wavs/Motorcycle.wav\", target_sample_rate=wav_sample_rate)\n",
    "wav2 = read_wav(\"wavs/Yakov_Golman_-_10_-_Valse.wav\", target_sample_rate=wav_sample_rate)\n",
    "\n",
    "wav1_time = len(wav1)/wav_sample_rate\n",
    "wav2_time = len(wav2)/wav_sample_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Audio sample length: \"+str(len(wav1)/wav_sample_rate)+\" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternatively, you could use scipy.io and its wavfile \n",
    "# (but we won't use this here, because there are troubles with playback in IPython)\n",
    "# see https://colab.research.google.com/drive/1--xY78_ZTFwpI7F2ZfaeyFKiAOG2nkwd\n",
    "\n",
    "from scipy.io import wavfile\n",
    "wav1_scipy = wavfile.read(\"wavs/Motorcycle.wav\")\n",
    "\n",
    "# Separete the object elements\n",
    "framerate = wav1_scipy[0]\n",
    "sounddata = wav1_scipy[1]\n",
    "time      = np.arange(0,len(sounddata))/framerate\n",
    "\n",
    "# Show information about the object\n",
    "print('Sample rate:',framerate,'Hz')\n",
    "print('Total time:',len(sounddata)/framerate,'s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a player for the (mono) sound, play it back\n",
    "from IPython.display import Audio\n",
    "Audio(wav1,rate=wav_sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### &lt;back to the presentation&gt;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize / represent the data\n",
    "\n",
    "#### 1) as a wave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable plotting inside the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlabels = [x/wav_sample_rate for x in range(len(wav1))]\n",
    "plt.figure(figsize=(16, 4))\n",
    "plt.plot(xlabels, wav1)\n",
    "plt.ylabel('amplitude')\n",
    "plt.xlabel('time [s]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# let's visualize just the first second\n",
    "plt.figure(figsize=(16, 4))\n",
    "plt.plot(xlabels[:wav_sample_rate], wav1[:wav_sample_rate])\n",
    "plt.ylabel('amplitude')\n",
    "plt.xlabel('time [s]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and the last second\n",
    "plt.figure(figsize=(16, 4))\n",
    "plt.plot(xlabels[-wav_sample_rate:], wav1[-wav_sample_rate:])\n",
    "plt.ylabel('amplitude')\n",
    "plt.xlabel('time [s]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### &lt;back to the presentation&gt;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) as spectrum and phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First compute fourier transform. Below is code that computes FFT for an example series - a 5Hz sine wave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspired by https://plot.ly/matplotlib/fft/\n",
    "\n",
    "# first show FFT of a 5Hz signal, duration 1 second:\n",
    "five_hz_sine_sample_rate = 400.0;  # sampling rate\n",
    "ts = 1.0/five_hz_sine_sample_rate; # sampling interval\n",
    "t = np.arange(0,1,ts) # time vector\n",
    "\n",
    "ff = 5;   # frequency of the signal\n",
    "five_hz_sine = np.sin(2*np.pi*ff*t)\n",
    "\n",
    "def compute_fft(data, times, sampling_rate):\n",
    "    n = len(data)\n",
    "    total_time = n/sampling_rate\n",
    "    frq = sp.linspace(0, sampling_rate, n)\n",
    "    frq = frq[range(n//2)] # one side frequency range\n",
    "\n",
    "    fft = np.fft.fft(data)/n # fft computing and normalization\n",
    "    fft = fft[range(n//2)]\n",
    "\n",
    "    fig, ax = plt.subplots(3, 1, figsize=(8, 10))\n",
    "    ax[0].plot(times,data)\n",
    "    ax[0].set_xlabel('Time')\n",
    "    ax[0].set_ylabel('Amplitude')\n",
    "    ax[1].plot(frq,abs(fft),'r') # plotting the magnitudes\n",
    "    ax[1].set_xlabel('Freq (Hz)')\n",
    "    ax[1].set_ylabel('magnitude')\n",
    "    ax[2].plot(frq,np.angle(fft),'g') # plotting the phase\n",
    "    ax[2].set_xlabel('Freq (Hz)')\n",
    "    ax[2].set_ylabel('phase')\n",
    "\n",
    "    plt.show()\n",
    "    # note that the frequency is 0.5 and not 1 since we're showing \n",
    "    # only the positive frequencies of the FFT\n",
    "\n",
    "    print(\"maximum frequency magnitude: \" + str(np.amax(abs(fft))) + \n",
    "          \" at frequency: \"+str(np.argmax(abs(fft))/total_time) + \" Hz and \" +\n",
    "          \"phase: \"+str(np.angle(fft[np.argmax(abs(fft))])))\n",
    "    print(\"value of FT at the maximum magnitude: \"+str(fft[np.argmax(abs(fft))]))\n",
    "    \n",
    "    return fft\n",
    "    \n",
    "example_fft = compute_fft(five_hz_sine, t, five_hz_sine_sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is the fft of our sound sample. Notice the peak at the low frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav1_fft = compute_fft(wav1, xlabels, wav_sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### &lt;back to the presentation&gt;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spectrum of a signal expressed using color coding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot the spectrogram magnitude data as a 1-D color-coded graph:\n",
    "\n",
    "xx, yy = sp.meshgrid(\n",
    "    sp.linspace(0,1, 2),\n",
    "    sp.linspace(0,wav_sample_rate//2, len(wav1_fft))\n",
    "    )\n",
    "\n",
    "zz = sp.zeros(xx.shape)\n",
    "for i in range(xx.shape[0]):\n",
    "    for j in range(xx.shape[1]):\n",
    "        zz[i,j] = abs(wav1_fft[i])\n",
    "\n",
    "# plot the calculated function values\n",
    "fig, ax = plt.subplots(1,figsize=(1, 8),sharey='all')\n",
    "\n",
    "p = ax.pcolormesh(xx,yy,zz,cmap='viridis')\n",
    "\n",
    "# and a color bar to show the correspondence between function value and color\n",
    "fig.colorbar(p)\n",
    "\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the calculated function values, log values\n",
    "fig, ax = plt.subplots(1,figsize=(1, 8),sharey='all')\n",
    "\n",
    "p = ax.pcolormesh(xx,yy,np.log(zz+0.00001),cmap='viridis')\n",
    "\n",
    "# and a color bar to show the correspondence between function value and color\n",
    "fig.colorbar(p)\n",
    "\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) as a spectrogram \n",
    "\n",
    "fft computed over short-length time windows, a.k.a. short-time FT (STFT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from code.lib.mel_features import spectrogram\n",
    "\n",
    "def print_spectrogram(spect, time, sample_rate, fig=None, ax=None, show_bar=True):\n",
    "    xx, yy = sp.meshgrid(\n",
    "        sp.linspace(0,time, len(spect)),\n",
    "        sp.linspace(0,sample_rate/2, len(spect[0]))\n",
    "        )\n",
    "\n",
    "    zz = sp.zeros(xx.shape)\n",
    "    for i in range(xx.shape[1]):\n",
    "        for j in range(xx.shape[0]):\n",
    "            zz[j,i] = spect[i,j]\n",
    "\n",
    "    if fig is None:\n",
    "        # plot the calculated function values\n",
    "        fig, ax = plt.subplots(1,figsize=(8, 8))\n",
    "    \n",
    "    p = ax.pcolormesh(xx,yy,zz)  # can use different colormaps, e.g. cmap='jet'\n",
    "\n",
    "    if show_bar:\n",
    "        # and a color bar to show the correspondence between function value and color\n",
    "        fig.colorbar(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spect = spectrogram(five_hz_sine, sample_rate=five_hz_sine_sample_rate, logarithmic=True, window_length_secs=0.1)\n",
    "# notice how the window length_secs has an effect on the localization of frequencies. Try using 0.5 or 0.9\n",
    "\n",
    "print_spectrogram(spect, 1.0, five_hz_sine_sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spect = spectrogram(wav1, sample_rate=wav_sample_rate, logarithmic=False)\n",
    "  \n",
    "print_spectrogram(spect, wav1_time, wav_sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spect = spectrogram(wav1, sample_rate=wav_sample_rate, logarithmic=True)\n",
    "\n",
    "print_spectrogram(spect, wav1_time, wav_sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### &lt;back to the presentation&gt;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) as a mel spectrogram (computes a spectrogram, then converts it to mel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from code.lib.mel_features import mel_spectrogram\n",
    "\n",
    "mel_sp = mel_spectrogram(wav1, \n",
    "                         sample_rate=wav_sample_rate, \n",
    "                         log_offset=0.01, \n",
    "                         window_length_secs=0.1, \n",
    "                         hop_length_secs=0.01, \n",
    "                         logarithmic=True,\n",
    "                         num_mel_bins=256,\n",
    "                         lower_edge_hertz=10,\n",
    "                         upper_edge_hertz=8000)\n",
    "\n",
    "print_spectrogram(mel_sp, wav1_time, wav_sample_rate)\n",
    "# warning: the numbers on the Y axis no longer match the real frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool visualizations may be achieved using external tools, like [Sonic Visualizer](https://www.sonicvisualiser.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### &lt;back to the presentation&gt;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5) as a wavelet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from code.lib.wavelet_transform import continuous_wavelet_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wavelet transform on the 5Hz signal:\n",
    "wavelet_five_hz = continuous_wavelet_transform(five_hz_sine)\n",
    "wavelet_five_hz_print = np.transpose(np.flipud(wavelet_five_hz))\n",
    "\n",
    "#hack to make the data visible in the plot (it's 1D otherwise):\n",
    "wavelet_five_hz_print = wavelet_five_hz_print + wavelet_five_hz_print\n",
    "\n",
    "print_spectrogram(wavelet_five_hz_print + wavelet_five_hz_print, 1.0, five_hz_sine_sample_rate)\n",
    "\n",
    "#ignore the future warning below, it has been fixed in the newest fftpack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The y labels are not accurate - they no longer refer to frequency, but to scale and position used by the wavelet transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warning: This uses a lot of RAM!\n",
    "\n",
    "# wavelet_wav = continuous_wavelet_transform(wav1)\n",
    "\n",
    "# wavelet_wav_print = np.transpose(np.flipud(wavelet_wav))\n",
    "\n",
    "\n",
    "# print_spectrogram(wavelet_wav_print, wav1_time, wav_sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### &lt;back to the presentation&gt;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction\n",
    "\n",
    "An example using pyAudioAnalysis library [for feature extraction](https://github.com/tyiannak/pyAudioAnalysis/wiki/3.-Feature-Extraction). For higher-performance models, we'd recommend feature extraction using VGGish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from code.lib.pyAudioAnalysis import audioBasicIO\n",
    "from code.lib.pyAudioAnalysis import audioFeatureExtraction\n",
    "\n",
    "Fs = wav_sample_rate\n",
    "data = wav1\n",
    "\n",
    "features, f_names = audioFeatureExtraction.stFeatureExtraction(data, Fs, 0.050*Fs, 0.025*Fs);\n",
    "# features now contain an array of 32 standard audio features\n",
    "# see https://github.com/tyiannak/pyAudioAnalysis/wiki/3.-Feature-Extraction\n",
    "plt.subplot(2,1,1); plt.plot(features[0,:]); plt.xlabel('Frame no'); plt.ylabel(f_names[0]); \n",
    "plt.subplot(2,1,2); plt.plot(features[1,:]); plt.xlabel('Frame no'); plt.ylabel(f_names[1]); \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VGGish feature extraction. For more information, check out [this colab notebook](https://colab.research.google.com/drive/1TbX92UL9sYWbdwdGE0rJ9owmezB-Rl1C#scrollTo=9BKF-1dzDhnz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# first make sure your vggish is installed correctly\n",
    "from code.lib.vggish.vggish_smoke_test import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go through the smoke test bit by bit, because it actually loads a trained network and uses it to compute an embedding of a test signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2017 The TensorFlow Authors All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "# modified by GoodAI, 2019\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import code.lib.vggish.vggish_input as vggish_input\n",
    "import code.lib.vggish.vggish_params as vggish_params\n",
    "import code.lib.vggish.vggish_postprocess as vggish_postprocess\n",
    "import code.lib.vggish.vggish_slim as vggish_slim\n",
    "\n",
    "# Paths to downloaded VGGish files.\n",
    "checkpoint_path = 'code/lib/vggish/vggish_model.ckpt'\n",
    "pca_params_path = 'code/lib/vggish/vggish_pca_params.npz'\n",
    "\n",
    "# Relative tolerance of errors in mean and standard deviation of embeddings.\n",
    "rel_error = 0.1  # Up to 10%\n",
    "\n",
    "# Generate a 1 kHz sine wave at 44.1 kHz (we use a high sampling rate\n",
    "# to test resampling to 16 kHz during feature extraction).\n",
    "num_secs = 3\n",
    "freq = 1000\n",
    "sr = 44100\n",
    "t = np.linspace(0, num_secs, int(num_secs * sr))\n",
    "x = np.sin(2 * np.pi * freq * t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vggish_params.EXAMPLE_HOP_SECONDS = 0.01  # hop by 0.01s\n",
    "\n",
    "# Produce a batch of log mel spectrogram examples.\n",
    "input_batch = vggish_input.waveform_to_examples(x, sr)\n",
    "#input_batch = vggish_input.waveform_to_examples(wav1[0:160000], wav_sample_rate)  # only 10 seconds to limit memory\n",
    "\n",
    "# Define VGGish, load the checkpoint, and run the batch through the model to\n",
    "# produce embeddings.\n",
    "with tf.Graph().as_default(), tf.Session() as sess:\n",
    "    vggish_slim.define_vggish_slim()\n",
    "    vggish_slim.load_vggish_slim_checkpoint(sess, checkpoint_path)\n",
    "\n",
    "    features_tensor = sess.graph.get_tensor_by_name(\n",
    "        vggish_params.INPUT_TENSOR_NAME)\n",
    "    embedding_tensor = sess.graph.get_tensor_by_name(\n",
    "        vggish_params.OUTPUT_TENSOR_NAME)\n",
    "    [embedding_batch] = sess.run([embedding_tensor],\n",
    "                                 feed_dict={features_tensor: input_batch})\n",
    "    # print('VGGish embedding: ', embedding_batch[0])\n",
    "\n",
    "    # Postprocess the results to produce whitened quantized embeddings.\n",
    "    pproc = vggish_postprocess.Postprocessor(pca_params_path)\n",
    "    postprocessed_batch = pproc.postprocess(embedding_batch)\n",
    "\n",
    "print('Postprocessed VGGish embedding (first item): ', postprocessed_batch[0], '\\ntotal items: ', len(postprocessed_batch))\n",
    "\n",
    "# End of modified vggish_smoke_test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 203 items for a 3-second audio, because an embedding is computed for 0.96s slices with hop length equal to 0.01s and minimal sample length equal to 0.98s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### &lt;back to the presentation&gt;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM, SVM+RBF, RandomForest, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from code.lib.pyAudioAnalysis import audioTrainTest as aT\n",
    "\n",
    "# calls sklearn.svm.SVC with a linear kernel or an rbf kernel\n",
    "# or sklearn.ensemble.RandomForestClassifier\n",
    "\n",
    "wav1_slices = [wav1[i*16000:(i+1)*16000] for i in range(0,20)]\n",
    "wav2_slices = [wav2[i*16000:(i+1)*16000] for i in range(0,20)]\n",
    "aT.featureWavAndTrain([wav1_slices, wav2_slices],\n",
    "                      wav_sample_rate,\n",
    "                      [\"motorcycle\",\"piano\"],\n",
    "                      1.0, \n",
    "                      1.0, \n",
    "                      aT.shortTermWindow, \n",
    "                      aT.shortTermStep, \n",
    "                      \"svm\", # or \"svm_rbf\" or \"randomforest\" or other...\n",
    "                      \"svmMusicGenre3\", \n",
    "                      True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### &lt;back to the presentation&gt;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_spectrogram(spectrogram, width):\n",
    "    right = width\n",
    "    slices = []\n",
    "    while right <= spectrogram.shape[0]:\n",
    "        slices.append(spectrogram[right-width:right, :])\n",
    "        right += width\n",
    "    \n",
    "    return slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_sp2 = mel_spectrogram(wav2, \n",
    "                         sample_rate=wav_sample_rate, \n",
    "                         log_offset=0.01, \n",
    "                         window_length_secs=0.1, \n",
    "                         hop_length_secs=0.01, \n",
    "                         logarithmic=True,\n",
    "                         num_mel_bins=256,\n",
    "                         lower_edge_hertz=10,\n",
    "                         upper_edge_hertz=8000)\n",
    "print_spectrogram(mel_sp2, wav2_time, wav_sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create slices of the whole-sample spectrogram.\n",
    "slices_1 = slice_spectrogram(mel_sp, width=256)\n",
    "slices_2 = slice_spectrogram(mel_sp2, width=256)\n",
    "\n",
    "all_slices = slices_1 + slices_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a couple of slices.\n",
    "n_to_show = 4\n",
    "fig, ax = plt.subplots(1, n_to_show, sharey=True, figsize=(14, 8)) #, figsize=(8, 10))\n",
    "for i, (s, ax_) in enumerate(zip(slices_2[:n_to_show], ax)):\n",
    "    print_spectrogram(s, 256, wav_sample_rate, fig, ax_, show_bar=i==0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### &lt;back to the presentation&gt;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully connected single layer feedforward network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features for the network.\n",
    "\n",
    "def extract_features(slice_data, n_features, eps=0.0001):\n",
    "    # Normalize the slice to (mean = 0, std = 1).\n",
    "    data_std = max(np.std(slice_data, ddof=1), eps)\n",
    "    \n",
    "    slice_data = (slice_data - np.mean(slice_data)) / data_std\n",
    "    \n",
    "    # Get frequencies present in each line of the spectrogram (rfft computes dft on real input).\n",
    "    freqs = np.fft.fft(slice_data)\n",
    "    \n",
    "    # Get magnitude of the frequencies present (absolute value of a complex number).\n",
    "    # rfft returns the result multiplied by the shape, divide.\n",
    "    magnitudes = np.abs(freqs) / slice_data.shape[1]\n",
    "    \n",
    "    # Take only first n_features.\n",
    "    result = magnitudes[..., :n_features].astype(np.float32)\n",
    "    return result\n",
    "\n",
    "n_features = 20\n",
    "data = [extract_features(slice_data, n_features) for slice_data in all_slices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the labels.\n",
    "labels_1 = [0 for _ in range(len(slices_1))]\n",
    "labels_2 = [1 for _ in range(len(slices_2))]\n",
    "\n",
    "labels = labels_1 + labels_2\n",
    "\n",
    "n_classes = len(set(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create the train/test split.\n",
    "indices = list(range(len(data)))\n",
    "\n",
    "# Q: Notice a problem here?\n",
    "training_indices, testing_indices = train_test_split(indices, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "training_data = torch.tensor(np.array([data[i] for i in training_indices]))\n",
    "training_labels = torch.tensor(np.array([labels[i] for i in training_indices]), dtype=torch.long)\n",
    "testing_data = torch.tensor(np.array([data[i] for i in testing_indices]))\n",
    "testing_labels = torch.tensor(np.array([labels[i] for i in testing_indices]), dtype=torch.long)\n",
    "\n",
    "training_dataset = TensorDataset(training_data, training_labels)\n",
    "testing_dataset = TensorDataset(testing_data, testing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class weights for the loss function.\n",
    "class_weights = [len(cls_labels)/len(labels) for cls_labels in (labels_1, labels_2)]\n",
    "print(f\"Class weights:\\t{class_weights}\")\n",
    "print(f\"Testing labels:\\t{testing_labels}\")\n",
    "\n",
    "class_weights_tensor = torch.tensor(class_weights, device=device, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnectedNet(torch.nn.Module):\n",
    "    \"\"\"Fully connected feed-forward neural network. \"\"\"\n",
    "    def __init__(self, input_height: int, n_classes: int, n_features: int):\n",
    "        super().__init__()\n",
    "\n",
    "        # Create the fully connected layer.\n",
    "        self.output = torch.nn.Linear(input_height * n_features, n_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        data = data.view(data.size(0), -1)  # View as (batch_size, n_features).\n",
    "        out = self.output(data)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FullyConnectedNet(input_height=256, n_classes=n_classes, n_features=n_features)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, criterion, optimizer, training_dataset):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    training_loader = DataLoader(dataset=training_dataset, batch_size=5, shuffle=True)\n",
    "    \n",
    "    with tqdm(training_loader) as progress_bar:\n",
    "        for slices, labels in progress_bar:\n",
    "            slices = slices.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(slices)\n",
    "\n",
    "            # The criterion is CrossEntropyLoss, which is log-softmax and negative log-likelihood.\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, testing_dataset):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    testing_loader = DataLoader(dataset=testing_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    softmax = torch.nn.Softmax(dim=1)\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    for slices, labels in testing_loader:\n",
    "        slices = slices.to(device)\n",
    "\n",
    "        labels = labels.cpu().numpy()\n",
    "        outputs = softmax(model(slices)).detach().cpu().numpy()\n",
    "        \n",
    "        all_predictions.extend(outputs)\n",
    "        all_labels.extend(labels)\n",
    "\n",
    "    return all_predictions, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, criterion, optimizer, training_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_result(all_predictions, all_labels):\n",
    "    best_guess = np.argmax(all_predictions, axis=1)\n",
    "\n",
    "    print(f\"Predicted:\\t{best_guess}\")\n",
    "    print(f\"Ground truth:\\t{np.array(all_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions, all_labels = test(model, testing_dataset)\n",
    "print(\"Predictions:\")\n",
    "for p in all_predictions:\n",
    "    print(p)\n",
    "    \n",
    "print()\n",
    "print_result(all_predictions, all_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### &lt;back to the presentation&gt;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet - transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet is originally trained on images with 3 channels (RGB). We will simply expand the one channel that we have.\n",
    "slices_expanded = [torch.tensor(s, dtype=torch.float32).view(1, 256, 256).expand(3, 256, 256) for s in all_slices]\n",
    "\n",
    "training_images = torch.stack([slices_expanded[i] for i in training_indices])\n",
    "testing_images = torch.stack([slices_expanded[i] for i in testing_indices])\n",
    "\n",
    "training_dataset_images = TensorDataset(training_images, training_labels)\n",
    "testing_dataset_images = TensorDataset(testing_images, testing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.resnet import resnet18\n",
    "\n",
    "resnet = resnet18(pretrained=True)\n",
    "resnet.fc = torch.nn.Linear(resnet.fc.in_features, n_classes)\n",
    "\n",
    "resnet.avgpool = torch.nn.AdaptiveAvgPool2d((1, 1))  # Fix copied from torchvision master.\n",
    "\n",
    "optimizer = torch.optim.SGD(resnet.parameters(), lr=0.1, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for _ in range(2):\n",
    "    # The criterion is the same as before - class-weighted cross entropy loss.\n",
    "    train(resnet, criterion, optimizer, training_dataset_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions, all_labels = test(resnet, testing_dataset_images)\n",
    "print_result(all_predictions, all_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### &lt;back to the presentation&gt;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=4)\n",
    "splits = kfold.split(iris.data, iris.target)\n",
    "\n",
    "# We're going to collect per-class recall values.\n",
    "recalls = []\n",
    "for train_indices, test_indices in splits:\n",
    "    X_train, y_train = iris.data[train_indices], iris.target[train_indices]\n",
    "    X_test, y_test = iris.data[test_indices], iris.target[test_indices]\n",
    "    \n",
    "    model = FullyConnectedNet(input_height=256, n_classes=n_classes, n_features=n_features)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.4)\n",
    "    \n",
    "    # Train and test the model.\n",
    "    # The criterion is the same as before - class-weighted cross entropy loss.\n",
    "    train(model, criterion, optimizer, training_dataset)\n",
    "    all_predictions, all_labels = test(model, testing_dataset)\n",
    "    \n",
    "    print_result(all_predictions, all_labels)\n",
    "    \n",
    "    # Calculate per-class recall.\n",
    "    cm = confusion_matrix(np.array(all_labels), np.argmax(all_predictions, axis=1))\n",
    "    per_class_rec = []\n",
    "    for c in range(n_classes):\n",
    "        recall = cm[c][c] / cm[c].sum()  # True positives/all samples from c.\n",
    "        per_class_rec.append(recall)\n",
    "        \n",
    "    recalls.append(per_class_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for class_recalls in recalls:\n",
    "    print(class_recalls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = []\n",
    "for c in range(n_classes):\n",
    "    recs = [rec[c] for rec in recalls]\n",
    "    print(f\"Class {c}:\")\n",
    "    mean = np.mean(recs)\n",
    "    means.append(mean)\n",
    "    print(f\"\\tRecall mean:\\t\\t{mean}\")\n",
    "    print(f\"\\tRecall stddev:\\t{np.std(recs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Class weights: {class_weights}\")\n",
    "class_weighted_recall = sum(mean*weight for mean, weight in zip(means, class_weights))\n",
    "print(f\"Class-weighted recall: {class_weighted_recall}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### &lt;back to the presentation&gt;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
